{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3707eadf",
   "metadata": {},
   "source": [
    "# Lab | Customer Analysis Final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147b558f",
   "metadata": {},
   "source": [
    "## Problem (case study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f98edc",
   "metadata": {},
   "source": [
    "Input of this analyses is data from marketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Description\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model           # skearn is for machine learning \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import math                       \n",
    "import warnings                  \n",
    "warnings.filterwarnings('ignore')\n",
    "#in_file = 'marketing_customer_analysis.csv'        # official data for the Lab\n",
    "in_file = 'marketing_customer_analysis_round2.csv'  # with data to be standardized\n",
    "customers = pd.DataFrame(pd.read_csv(in_file))\n",
    "print('Input: marketing_customer_analysis.csv')\n",
    "print('Number of rows:', customers.shape[0])\n",
    "print('Number of columns', customers.shape[1])\n",
    "print(' ')\n",
    "print(customers.dtypes)\n",
    "pd.set_option('display.max_columns', None)  # all columns in jupyter\n",
    "display(customers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c63f9",
   "metadata": {},
   "source": [
    "### Goal\n",
    "The goal of the analysis is to create a model to predict if behaviour of customers with certain characteristics. The objective of the underlying data is to understand customer demographics and buying behavior. In the data we can find several information about the customers, such as: where they live, if they responded to earlier campaigns, personal information and customer value information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e27b4",
   "metadata": {},
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a91510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the .csv file\n",
    "# done in previous chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b609a89",
   "metadata": {},
   "source": [
    "## Cleaning/Wrangling/EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78015c",
   "metadata": {},
   "source": [
    "<details><summary>▶ Theory: 10 ways to drop columns in dataframes</summary>\n",
    "<p>\n",
    "\n",
    "### 10 Different ways to drop columns:\n",
    "**Delete with del**<br />\n",
    "<span style=\"color:#444; font-family: 'Courier'; background-color: #F7F7F7\">\n",
    "del df['Locations']</span>\n",
    "    \n",
    "**Drop the label 'Locations'**<br />\n",
    "<span style=\"color:#444; font-family: 'Courier'; background-color: #F7F7F7\">\n",
    "df.drop(labels='Locations', axis=1)</span>\n",
    "    \n",
    "**With columns parameter**<br /> \n",
    "The value of the axis parameter need not be passed.<br />\n",
    "<span style=\"color:#444; font-family: 'Courier'; background-color: #F7F7F7\">\n",
    "df.drop(columns='Founder')</span>\n",
    "    \n",
    "**Multiple columns**<br />\n",
    "<span style=\"color:#444; font-family: 'Courier'; background-color: #F7F7F7\">\n",
    "df.drop(labels=['Locations', 'Founder'], axis=1)</span>\n",
    "    \n",
    "**Pass a list of column names to the columns parameter**<br />\n",
    "<span style=\"color:#444; font-family: 'Courier'; background-color: #F7F7F7\">\n",
    "df.drop(columns=['Founder', 'Locations'])</span>\n",
    "    \n",
    "**drop multiple columns, using an index**<br />\n",
    "<span style=\"color:#444; font-family: 'Courier'; background-color: #F7F7F7\">\n",
    "df.drop(df.columns[[1, 3]], axis=1)</span>\n",
    "\n",
    "**drop multiple columns, using index as variable**<br />\n",
    "<span style=\"color:#444; font-family: 'Courier'; background-color: #F7F7F7\">\n",
    "drop_i = Index(['Name', 'Locations', 'States'], dtype='object')< br/>\n",
    "df.drop(df.columns[drop_i], axis=1)</span>\n",
    "    \n",
    "**Pass column names to the loc indexing method**<br />\n",
    "<span style=\"color:#444; font-family: 'Courier'; background-color: #F7F7F7\">\n",
    "df.drop(df.loc[:, ['Locations', 'Founder']], axis=1)</span>\n",
    "\n",
    "**Drop columns with certain patterns using .loc method**<br />\n",
    "You can also pass name patterns as label names to the loc index. Using name patterns, you can remove all the columns from a DataFrame which have the specified pattern in them. Function .startswith() is a string function which is used to check if a string starts with the specified character or not.<br />\n",
    "<span style=\"color:#444; font-family: 'Courier'; background-color: #F7F7F7\">\n",
    "df.drop(df.loc[:, df.columns[df.columns.str.startswith('F')]], axis=1)</span>\n",
    "    \n",
    "**Drop tables, EXCEPT the ones**<br />\n",
    "And pass the column names which are to be retained<br />\n",
    "<span style=\"color:#444; font-family: 'Courier'; background-color: #F7F7F7\">\n",
    "df.drop(df.columns.difference(['Name', 'States', 'Founding Year']), axis=1)</span>\n",
    "    \n",
    "**The pop method**<br />\n",
    "Is used to remove the specified column from the DataFrame and return the removed column as a pandas Series. Pass the name of the column which is to be removed and return it as a pandas Series.<br />\n",
    "<span style=\"color:#444; font-family: 'Courier'; background-color: #F7F7F7\">\n",
    "founder = df.pop('Founder')<br />\n",
    "print(founder)<br />\n",
    "print('\\n')  # Escape character to print an empty new line<br />\n",
    "print(df)<br /></span>\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change headers names\n",
    "customers.columns = customers.columns.map(lambda x : x.replace(\"-\", \"_\").replace(\" \", \"_\"))\n",
    "# still not satisfied with EmploymentStatus\n",
    "customers = customers.rename(columns={\"EmploymentStatus\":\"Employment_Status\"})\n",
    "# Drop the first column\n",
    "customers = customers.drop(labels='Unnamed:_0', axis=1)\n",
    "customers.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02066f17",
   "metadata": {},
   "source": [
    "### Deal with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fcf017",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Deal with NaN values (1)\n",
    "# Apparently there are no records in the input file that are without value. \n",
    "# To illustrate and to make it more fun, I have added the input file from \n",
    "# round 2 to the final Lab. This one is called \n",
    "# marketing_customer_analysis_round2.csv\n",
    "print(customers.isna().sum())\n",
    "print('Number of rows:', customers.shape[0])\n",
    "print('Number of columns', customers.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62772f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with NaN values (2)\n",
    "\n",
    "# Column State: fill NaN value of a certain row with a given value\n",
    "customers['State'] = customers['State'].fillna('USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98678eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with NaN values (3)\n",
    "\n",
    "# Colum Response: fill boolean column with No (sounds like least possible consequence)\n",
    "# display(customers[customers['Response'].isna()==True].head(60))    # test line to see some values\n",
    "# customers.loc[:,['Response']].head(60)                             # alternative test line\n",
    "customers['Response'] = customers['Response'].fillna('No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with NaN values (4)\n",
    "\n",
    "# Column Months_Since_Last_Claim: fill the NaN fields of this column with mean value of the rest\n",
    "mean_Months_Since_Last_Claim = round(customers['Months_Since_Last_Claim'].mean(),1)\n",
    "customers['Months_Since_Last_Claim'] = customers['Months_Since_Last_Claim'].fillna(mean_Months_Since_Last_Claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with NaN values (5)\n",
    "\n",
    "# Column Number_of_Open_Complaints: with mean value rounded whole numbers\n",
    "mean_Number_of_Open_Complaints = round(customers['Number_of_Open_Complaints'].mean(),0)\n",
    "customers['Number_of_Open_Complaints'] = customers['Number_of_Open_Complaints'].fillna(mean_Number_of_Open_Complaints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6474370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with NaN values (6)\n",
    "\n",
    "# Columns Vehicle_Class and Vehicle_Size \n",
    "# it seems like both columns have NaN together so I treat them equally\n",
    "# customers.loc[86:100,['Vehicle_Class','Vehicle_Size']].head(60)\n",
    "# want to know most common Vehicle_Size\n",
    "# customers['Vehicle_Size'].unique()\n",
    "# aparently 3 options L / M / S\n",
    "# customers['Vehicle_Size'].value_counts()\n",
    "# decide to fill Vehicle_Size with Medsize\n",
    "customers['Vehicle_Size'] = customers['Vehicle_Size'].fillna('Medsize')\n",
    "# customers['Vehicle_Class'].value_counts()\n",
    "# decide to fill Vehicle_Size with Four-Door Car\n",
    "customers['Vehicle_Class'] = customers['Vehicle_Class'].fillna('Four-Door Car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d229a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with NaN values (7)\n",
    "\n",
    "# Column Vehicle_Class | Vehicle_Size | Vehicle_Type\n",
    "# pd.set_option('display.max_columns', None)    # see all columns, since Months_Since_Last_Claim was hidden\n",
    "# customers[customers['Vehicle_Size'].isna()==True].head(60)    # test line to see some values\n",
    "# seems that all NaN on Class and Size are vehicle type A\n",
    "# customers.loc[customers['Vehicle_Type'] == 'A']\n",
    "# customers.loc[customers['Vehicle_Size'] == 'A']\n",
    "# customers.loc[:,['Vehicle_Class','Vehicle_Size','Vehicle_Type']].head(60)\n",
    "# Looking at Vehicle_Type seems that this field is really bad on data quality,\n",
    "# since it's either A or NaN. It doesn't really give more or better info than Vehicle_Class \n",
    "# and Vehicle_Size together, therefore:\n",
    "# Decide to eliminate column Vehicle_Type\n",
    "exist_vt = \"Vehicle_Type\" in customers                    # avoid errors at re-run code\n",
    "if exist_vt==True:\n",
    "    customers = customers.drop(['Vehicle_Type'], axis=1)  # drop a single column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with NaN values (Final check)\n",
    "round(customers.isna().sum()/len(customers),4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ea0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Features\n",
    "cat_cols = customers.select_dtypes('object').columns\n",
    "print('Categorical Features in this dataframe are:')\n",
    "print(' ')\n",
    "for col in cat_cols:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b881e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Features\n",
    "print('Numerical Features in this dataframe are:')\n",
    "print(' ')\n",
    "num_cols = customers._get_numeric_data()\n",
    "for col in num_cols:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca69df1",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Exploratory data analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods: for seeing what the data can tell us beyond the formal modeling or hypothesis testing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee4d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Use some of these to explore:\n",
    "#\n",
    "# customers.info(memory_usage='deep')\n",
    "# print(customers.shape)\n",
    "# display(customers.head(60))\n",
    "# display(customers)\n",
    "# Exploratory Data Analysis (1)\n",
    "# \n",
    "# Explore 1: Effective_To_Date - convert this column to a real date? \n",
    "# Explore 2: Income - what do we do with odd incomes (negative or 0)?\n",
    "#\n",
    "# Convert Effective_To_Date to date format\n",
    "customers['Effective_To_Date'].dtype\n",
    "customers['Effective_To_Date'] = pd.to_datetime(customers['Effective_To_Date'], errors='coerce')\n",
    "customers['Effective_To_Date'].dtype\n",
    "# \n",
    "# Put rows in seperate df where Income is 0 \n",
    "customers_Inc0 = customers.loc[customers.Income == 0]   # perhaps for later use as df\n",
    "# customers_Inc0  # 2787 rows × 24 columns\n",
    "customers = customers.loc[customers.Income > 0]\n",
    "# customers  # 8123 rows × 24 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75816e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: visualization on Response Rate\n",
    "%matplotlib inline\n",
    "\n",
    "sns.countplot('Response', data=customers)\n",
    "plt.ylabel('Total number of Response')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c63d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: visualization on Response by the sales channel\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot('Response', hue='Sales_Channel', data=customers)\n",
    "plt.ylabel('Response by Sales Channel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e5d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: visualization on Response Rate by the Total Claim Amount\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(y='Total_Claim_Amount' , x='Response', data=customers)\n",
    "plt.ylabel('Response by Total Claim Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02797c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: visualization on Response Rate by Income\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(y='Income' , x='Response', data=customers)\n",
    "plt.ylabel('Response by Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f117a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First rough exploration with the \"plot bomb\"\n",
    "sns.pairplot(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all numeric columns with a loop for displot\n",
    "# %%time\n",
    "df1 = customers.select_dtypes([int, float])\n",
    "for i, col in enumerate(df1.columns):\n",
    "    plt.figure(i)\n",
    "    sns.displot(x=col, data=df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f70d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all numeric columns with a loop for histplot\n",
    "df1 = customers.select_dtypes([int, float])\n",
    "for i, col in enumerate(df1.columns):\n",
    "    plt.figure(i)\n",
    "    sns.histplot(x=col, data=df1, color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444863af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual exploration of correlation with the heatmap\n",
    "correlations_matrix = customers.corr()\n",
    "sns.heatmap(correlations_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusions based on the correlation matrix:\n",
    "# Some correlations that stand out:\n",
    "    # Monthly_Premium_Auto <--> Customer_Lifetime_Value (0.4)\n",
    "    # Monthly_Premium_Auto <--> Total_Claim_Amount (0.63)\n",
    "    # total_claim_amount <--> Income (negative correlation -0.36)\n",
    "# The rest seems to be neutral\n",
    "# Regarding total_claim_amount it's interesting to study possibility\n",
    "# to predict with the use of Monthly_Premium_Auto and Income\n",
    "# There are no features to be dropped due to very high correlation\n",
    "# Since highest corr is 0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c57194",
   "metadata": {},
   "source": [
    "## Processing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b525543",
   "metadata": {},
   "source": [
    "### Dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b9a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with outliers\n",
    "shape_before = customers.shape    # checksum \n",
    "print(shape_before)\n",
    "# Check what happens with notmality when outliers are removed, for:\n",
    "# - Monthly_Premium_Auto \n",
    "# - Customer_Lifetime_Value\n",
    "# - Total_Claim_Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8079bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dealing with outliers - Monthly_Premium_Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be2193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.boxplot(x=customers['Monthly_Premium_Auto'])\n",
    "# plt.show()\n",
    "# # sns.displot(customers['Monthly_Premium_Auto'])\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec904d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iqr = np.percentile(customers['Monthly_Premium_Auto'],75) - np.percentile(customers['Monthly_Premium_Auto'],25)\n",
    "# upper_limit = np.percentile(customers['Monthly_Premium_Auto'],75) + 1.5*iqr\n",
    "# lower_limit = np.percentile(customers['Monthly_Premium_Auto'],25) - 1.5*iqr\n",
    "# print('Inter Quartile Range:', iqr, 'Lower Limit (25)', lower_limit, 'Upper Limit (75):', upper_limit)\n",
    "# # Next line is to actually remove the outliers for Monthly_Premium_Auto\n",
    "# customers = customers[(customers['Monthly_Premium_Auto']>lower_limit) & (customers['Monthly_Premium_Auto']<upper_limit)]\n",
    "# print('Customers without outliers in Monthly_Premium_Auto:', customers.shape, 'Outliers Deleted:', shape_before[0]-customers.shape[0] )\n",
    "# sns.displot(customers['Monthly_Premium_Auto'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On closer inspection, however, these data do not provide a good normal distribution. \n",
    "# Decision: comment out the steps for dealing with outliers for Monthly_Premium_Auto\n",
    "# And re-run the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with outliers - Customer_Lifetime_Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.boxplot(x=customers['Customer_Lifetime_Value'])\n",
    "# plt.show()\n",
    "# sns.displot(customers['Customer_Lifetime_Value'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ff0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iqr = np.percentile(customers['Customer_Lifetime_Value'],75) - np.percentile(customers['Customer_Lifetime_Value'],25)\n",
    "# upper_limit = np.percentile(customers['Customer_Lifetime_Value'],75) + 1.5*iqr\n",
    "# lower_limit = np.percentile(customers['Customer_Lifetime_Value'],25) - 1.5*iqr\n",
    "# print('Inter Quartile Range:', iqr, 'Lower Limit (25)', lower_limit, 'Upper Limit (75):', upper_limit)\n",
    "# # # Next line is to actually remove the outliers for Monthly_Premium_Auto\n",
    "# customers = customers[(customers['Customer_Lifetime_Value']>lower_limit) & (customers['Customer_Lifetime_Value']<upper_limit)]\n",
    "# print('Customers without outliers in Monthly_Premium_Auto:', customers.shape, 'Outliers Deleted:', shape_before[0]-customers.shape[0] )\n",
    "# sns.displot(customers['Customer_Lifetime_Value'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e421d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On closer inspection, however, these data do not provide a good normal distribution. \n",
    "# Decision: comment out the steps for dealing with outliers for Customer_Lifetime_Value\n",
    "# And re-run the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with outliers - Total_Claim_Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfacc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=customers['Total_Claim_Amount'])\n",
    "plt.show()\n",
    "sns.displot(customers['Total_Claim_Amount'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr = np.percentile(customers['Total_Claim_Amount'],75) - np.percentile(customers['Total_Claim_Amount'],25)\n",
    "upper_limit = np.percentile(customers['Total_Claim_Amount'],75) + 1.5*iqr\n",
    "lower_limit = np.percentile(customers['Total_Claim_Amount'],25) - 1.5*iqr\n",
    "print('Inter Quartile Range:', iqr, 'Lower Limit (25)', lower_limit, 'Upper Limit (75):', upper_limit)\n",
    "# # Next line is to actually remove the outliers for Monthly_Premium_Auto\n",
    "customers = customers[(customers['Total_Claim_Amount']>lower_limit) & (customers['Total_Claim_Amount']<upper_limit)]\n",
    "print('Customers without outliers in Monthly_Premium_Auto:', customers.shape, 'Outliers Deleted:', shape_before[0]-customers.shape[0] )\n",
    "sns.displot(customers['Total_Claim_Amount'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike the outlier elimination operations for Monthly_Premium_Auto and Customer_Lifetime_Value, \n",
    "# this operation seem to have a good effect on Total_Claim_Amount\n",
    "customers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbf352c",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form to quantify the similarity of any pair of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8acc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of Total_Claim_Amount (1)\n",
    "sns.scatterplot(x=customers['Total_Claim_Amount'], y=customers['Monthly_Premium_Auto'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957814d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of Total_Claim_Amount (2)\n",
    "# First remove target from the columns\n",
    "y = customers['Monthly_Premium_Auto']\n",
    "X = customers.drop(['Monthly_Premium_Auto'], axis=1)\n",
    "X_num = X.select_dtypes(include = np.number)\n",
    "X_cat = X.select_dtypes(include = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e626b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of Total_Claim_Amount (3a)\n",
    "# Normalizing data: make data range from 0 - 1, instead of from min to max\n",
    "MinMax_transf = MinMaxScaler().fit(X_num)\n",
    "x_normalized = MinMax_transf.transform(X_num)\n",
    "print(x_normalized.shape)\n",
    "pd.DataFrame(x_normalized, columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd47ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of Total_Claim_Amount (3b)\n",
    "# scaling standard scaler: make data normal distributed with mean=0 and std=1\n",
    "standard_transf = StandardScaler().fit(X_num)\n",
    "x_standardized = standard_transf.transform(X_num)\n",
    "print(x_standardized.shape)\n",
    "pd.DataFrame(x_standardized, columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b7588",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "<br>\n",
    "\n",
    "<details><summary>▶ Theory Encoding</summary>\n",
    "<p>\n",
    "\n",
    "To convert categorical features to integer codes in order to use them in models, we can use different encoders. To consider:\n",
    "* One Hot Encoder\n",
    "* Label encoding\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "In this analysis we use One Hot Encoder, following these steps:\n",
    "* Step 1: separate the features from the labels (into y)\n",
    "* Step 2: separate the numeric columns (cust_X_num) from the categorical (cust_X_cat)\n",
    "* Step 3: Hot label Encoding serveral categorical columns via a for loop\n",
    "* Step 4: normalizing num data\n",
    "* Step 5: merging cust_X_num and cust_X_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Categorical Data - step 1\n",
    "# Separate the features from the labels\n",
    "cust_y = customers['Total_Claim_Amount']\n",
    "cust_X = customers.drop(['Total_Claim_Amount'], axis=1)\n",
    "# cust_X.head()\n",
    "# cust_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c1bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Categorical Data - step 2\n",
    "# Categorical features and numerical ones are going to be treated differently\n",
    "cust_X_num = X.select_dtypes(include = np.number)\n",
    "\n",
    "# In the R2 Model Validation appears that target Total_Claim_Amount should be eliminated, otherwise\n",
    "# there will be an R2 of 1.0 which is maximum score and not correct\n",
    "cust_X_num = cust_X_num.drop(labels=\"Total_Claim_Amount\", axis=1)\n",
    "\n",
    "cust_X_cat = X.select_dtypes(include = object)\n",
    "cust_X_cat = cust_X_cat.drop(columns=['Customer'])     # doesn't do anything useful and causes problems\n",
    "# cust_X_num.head()\n",
    "# cust_X_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba074dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Categorical Data - step 3\n",
    "# Hot label Encoding\n",
    "onehot_pd = pd.DataFrame()\n",
    "for column in cust_X_cat:\n",
    "    # print(\"Processing column:\", column)\n",
    "    cust_encoder = OneHotEncoder().fit(cust_X_cat[[column]])\n",
    "    cust_encoded = cust_encoder.transform(cust_X_cat[[column]]).toarray()\n",
    "    onehot_encoded = pd.DataFrame(cust_encoded, columns=cust_encoder.categories_)\n",
    "    onehot_pd = pd.concat([onehot_pd, onehot_encoded], axis=1)\n",
    "onehot_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Categorical Data - step 4\n",
    "# Normalizing num data (again)\n",
    "rene = MinMaxScaler().fit(cust_X_num)\n",
    "cust_X_num_n = rene.transform(cust_X_num)\n",
    "print(cust_X_num_n.shape)\n",
    "cust_X_num_n = pd.DataFrame(cust_X_num_n, columns=cust_X_num.columns)\n",
    "display(cust_X_num_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1561f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Categorical Data - step 5\n",
    "# merging X_num and X_cat\n",
    "cust_X_n = pd.concat([cust_X_num_n, onehot_pd], axis=1)\n",
    "cust_X_n.shape\n",
    "#display(cust_X_n.head(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e349ce",
   "metadata": {},
   "source": [
    "### Train set and test set splitting\n",
    "<br>\n",
    "\n",
    "<details><summary>▶ Theory Split-testing</summary>\n",
    "<p>\n",
    "\n",
    "Splitting into train-set and test-set: In order to not give our model the oportunity to cheat, it must accurately guess the values in the \"fresh\" dataset that it never saw before: X_train, X_test, y_train, y_test. Training is for building, testing is for validating. Naming conventions are # y_train en Y_set is meant as target. When Train is 80%, test is 20%. Random state gives THE SAME random set (for illustration in course), sometimes for testing. This could be any number this option is to fix the randomizer.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train set and test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(cust_X_n, cust_y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03376f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad76047",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372a742",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model.\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfeed58",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830522f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "predictions = lm.predict(X_train)\n",
    "print(\"R2 score:\", r2_score(y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_train, predictions)\n",
    "print('MSE'mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ccd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, SOMETHING IS WRONG HERE WITH THE MSE ... I CALL IT A DAY, BECAUSE I CAN'T SEE WHAT I DID WRONG\n",
    "# LACK OF EXPERIENCE WITH STATISTICS TO SOLVE THIS ON MY OWN - NEED SUPPORT!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a514885a",
   "metadata": {},
   "source": [
    "## Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccef85e",
   "metadata": {},
   "source": [
    "<details><summary>▶ Theory</summary>\n",
    "<p>\n",
    "\n",
    "Dit is een test\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers.shape)\n",
    "display(customers.head(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c30989",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(customers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
